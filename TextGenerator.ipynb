{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "qePMZMNbZvnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('EcoPreprocessed.csv')\n",
        "text = train_df['review']\n",
        "print('Number of training sentences: ',len(text))\n",
        "print(text[1])"
      ],
      "metadata": {
        "id": "TU_urKo-VXrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text.values)\n",
        "sequences = tokenizer.texts_to_sequences(text.values)\n",
        "print('One sequence of a sentence looks like this: ', sequences[0])\n",
        "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
        "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
        "words = []\n",
        "for sublist in sequences:\n",
        "  for item in sublist:\n",
        "    words.append(item)\n",
        "print(\"The words list first 10 elements: \", words[:10])\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "print(\"The number of different words: \", vocab_size)"
      ],
      "metadata": {
        "id": "CSVnf9isVcSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_len = 15\n",
        "pred_len = 1\n",
        "train_len = sentence_len - pred_len\n",
        "seq = []\n",
        "# Sliding window to generate train data\n",
        "for i in range(len(words)-sentence_len):\n",
        "    seq.append(words[i:i+sentence_len])\n",
        "print(\"The first two 15-length lists: \", seq[:2])\n",
        "\n",
        "# Reverse dictionary to decode tokenized sequences back to words\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Each row in seq is a 15 word long window. We append the first 14 words as the input to predict the 10th word\n",
        "trainX = []\n",
        "trainy = []\n",
        "for i in seq:\n",
        "    trainX.append(i[:train_len])\n",
        "    trainy.append(i[-1])"
      ],
      "metadata": {
        "id": "W-kL5YlZaRYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "3m63Yq-CeAj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(np.asarray(trainX),\n",
        "         pd.get_dummies(np.asarray(trainy)),\n",
        "         epochs = 200,\n",
        "         batch_size = 128)"
      ],
      "metadata": {
        "id": "Fe9pWWimn9Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model,seq,max_len = 15):\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds\n",
        "    # zeroes to the left side of our sequence until it becomes 14 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = pad_sequences(tokenized_sent[-14:],maxlen=14)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "\n",
        "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
      ],
      "metadata": {
        "id": "5M0sLTKp5UNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(model, \"sound\")"
      ],
      "metadata": {
        "id": "b-tgUGTQ3Zuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}